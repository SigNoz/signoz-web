---
date: 2024-06-06
title: Monitor Kafka Service
id: kafka
---

{/* This documentation provides a comprehensive, ready-to-use guide to set up client-side instrumentation and metrics collection for Kafka using OpenTelemetry and 
SigNoz. You will learn how to monitor brokers, topics, consumers, producers, and client interactions effectively. */}


{/* <figure data-zoomable align="center">
    <img
      src="/img/blog/2024/05/kafka-monitoring/request_flow.png"
      alt="A general request flow in a Kafka setup"
    />
    <figcaption>
      <i>
        General request flow in a Kafka setup
      </i>
    </figcaption>
</figure> */}


## Table of Contents

1. [Introduction](#introduction)
2. [Prerequisites](#prerequisites)
3. [Steps to Follow](#steps-to-follow)
    - [Step 1: Kafka Setup](#step-1-kafka-setup)
    - [Step 2: OpenTelemetry Java Agent Installation](#step-2-opentelemetry-java-agent-installation)
    - [Step 3: Java Producer-Consumer App Setup](#step-3-java-producer-consumer-app-setup)
    - [Step 4: OpenTelemetry Collector Setup](#step-4-opentelemetry-collector-setup)
    - [Step 5: Visualise Data in SigNoz](#step-5-visualize-data-in-signoz)


---

## Introduction

{/* Description about kafka and its different components */}

Kafka is a widely-used distributed messaging system designed to handle large volumes of real-time data. It is commonly employed for building scalable, 
fault-tolerant data pipelines and stream processing systems. Kafka allows producers to send messages to topics, which are partitioned to support scalability, 
and consumers to subscribe and process these messages asynchronously. 

Key components of Kafka include:

- **Producers**: Applications that publish messages to Kafka topics.
- **Consumers**: Applications that subscribe to topics and consume messages.
- **Brokers**: Kafka servers that manage data flow and ensure message persistence.
- **Topics**: Categories to which producers send messages and consumers subscribe to.
- **Partitions**: Divisions within a topic to ensure scalability and parallel processing.
- **Consumer Groups**: Groups of consumers that allow load distribution across multiple instances.
- **ZooKeeper/KRaft**: Manages coordination between brokers (in older Kafka versions, ZooKeeper is used; newer versions use KRaft).


In this documentation, we will guide you through setting up Kafka monitoring with OpenTelemetry and SigNoz. You'll learn how to instrument Kafka's 
components—producers, consumers, topics, and brokers—so that you can collect key metrics, traces and logs. This will help you gain deep insights into the 
performance and behavior of your Kafka cluster.



<figure data-zoomable align="center">
    <img
      src="/img/blog/2024/05/kafka-monitoring/kafka-otel-signoz.png"
      alt="Kafka integrated with OpenTelemetry for collecting metrics, traces and logs, visualized in SigNoz"
    />
    <figcaption>
      <i>
       Kafka integrated with OpenTelemetry for collecting metrics, traces and logs, visualized in SigNoz
      </i>
    </figcaption>
</figure>


---

## Prerequisites

Before you begin, ensure the following prerequisites are met:

1. **Java Development Kit (JDK)**: Ensure that JDK 8 or higher is installed on your system.
2. **Apache Kafka**: Kafka should be installed and running for your environment.
3. **Kafka Producer and Consumer Applications**: You will need Kafka producer and consumer applications in Java (Support for other languages will be added soon).
4. **Kafka Client Library**: Make sure the Kafka client libraries are included in your project. You can find the official Kafka client libraries from Apache [here](https://mvnrepository.com/artifact/org.apache.kafka/kafka-clients).
5. **OpenTelemetry Java Agent**: Download the OpenTelemetry Java auto-instrumentation agent JAR. Follow the setup guide from the official OpenTelemetry documentation [here](https://opentelemetry.io/docs/zero-code/java/agent/getting-started/) or directly download the latest agent JAR from [here](https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar).

Once these are set up, you will be ready to proceed with Kafka monitoring and instrumentation.


---

## Steps to Follow

This guide follows these primary steps:

1. [Kafka Setup](#step-1-kafka-setup)
2. [OpenTelemetry Java Agent Installation](#step-2-opentelemetry-java-agent-installation)
3. [Java Producer-Consumer App Setup](#step-3-java-producer-consumer-app-setup)
4. [SigNoz Setup](#step-4-signoz-setup)
5. [OpenTelemetry Collector Setup](#step-5-opentelemetry-collector-setup)

{/* For simplicity, Steps 1, 2, and 3 are assumed to be performed on the same host (VM, laptop, or containerized environment). */}


<Tabs>
<TabItem value="Local" label="Local / VM(AWS/GCP/Azure)" default>


### Step 1: Kafka Setup

#### 1.1 Kafka Installation

[Download the latest](https://kafka.apache.org/quickstart#quickstart_download) Kafka release and extract it.

For example, if the latest release is `2.13-3.7.0` then you can extract it using the follwing command:
```bash

# Extract the downloaded Kafka folder
tar -xzf kafka_2.13-3.7.0.tgz
cd kafka_2.13-3.7.0

```

#### 1.2 Start Kafka 

Kafka can be started using ZooKeeper or [KRaft](https://kafka.apache.org/quickstart#quickstart_startserver). We will use ZooKeeper:

```bash
bin/zookeeper-server-start.sh config/zookeeper.properties
```

#### 1.3 Configure and Start Brokers

Create two server properties files `s1.properties` and `s2.properties`to start two brokers:

<Admonition type="info">
Make sure you are inside the kafka folder that you extracted in the step before.
</Admonition>

```bash
# make sure you are inside the extracted kafka folder

cp config/server.properties config/s1.properties
cp config/server.properties config/s2.properties
```

Edit `s1.properties`:

```bash
vi config/s1.properties
```

Add the following configurations:

```
broker.id=1
listeners=PLAINTEXT://localhost:9092
log.dirs=/tmp/kafka_logs-1
zookeeper.connect=localhost:2181
```

Edit `s2.properties`:

```bash
vi config/s2.properties
```

Add the following configurations:

```
broker.id=2
listeners=PLAINTEXT://localhost:9093
log.dirs=/tmp/kafka_logs-2
zookeeper.connect=localhost:2181
```

Start Broker 1 with JMX port enabled:
```bash
JMX_PORT=2020 bin/kafka-server-start.sh config/s1.properties
```

Start Broker 2 in a new terminal with JMX port enabled:
```bash
JMX_PORT=2021 bin/kafka-server-start.sh config/s2.properties
```

#### 1.4 Create Kafka Topics

Create two Kafka topics:

```bash
bin/kafka-topics.sh --create --topic topic1 --bootstrap-server localhost:9092 --replication-factor 2 --partitions 2
bin/kafka-topics.sh --create --topic topic2 --bootstrap-server localhost:9092 --replication-factor 2 --partitions 1
```

#### 1.5 Verify Kafka Setup

List the Kafka topics and their partitions:

```bash
bin/kafka-topics.sh --describe --topic topic1 --bootstrap-server localhost:9092
bin/kafka-topics.sh --describe --topic topic2 --bootstrap-server localhost:9092
```

#### 1.6 Test Kafka Setup

**Produce Messages:**

```bash
bin/kafka-console-producer.sh --topic topic1 --bootstrap-server localhost:9092
```

Type some messages and press `Enter` to send.

**Consume Messages:**

Open a new terminal and run:

```bash
bin/kafka-console-consumer.sh --topic topic1 --from-beginning --bootstrap-server localhost:9092
```

You should see the messages you produced.

### Step 2: OpenTelemetry Java Agent Installation

#### 2.1 Java Agent Setup

Install the latest OpneTelemetry Java Agent:

```bash
wget https://github.com/open-telemetry/opentelemetry-java-instrumentation/releases/latest/download/opentelemetry-javaagent.jar
```

#### 2.2 (Optional) Configure OpenTelemetry Java Agent

Refer to the [OpenTelemetry Java Agent configurations](https://opentelemetry.io/docs/zero-code/java/agent/) for advanced setup options.


### Step 3: Java Producer-Consumer App Setup

#### 3.1 Running the Producer and Consumer Apps with OpenTelemetry Java Agent

Ensure you have Java and Maven installed.


**Compile your Java producer and consumer applications:** Ensure your producer and consumer apps are compiled and ready to run.

**Run Producer App with Java Agent:**

```bash
java -javaagent:/path/to/opentelemetry-javaagent.jar \
     -Dotel.service.name=producer-svc \
     -Dotel.traces.exporter=otlp \
     -Dotel.metrics.exporter=otlp \
     -Dotel.logs.exporter=otlp \
     -jar /path/to/your/producer.jar
```

**Run Consumer App with Java Agent:**

```bash
java -javaagent:/path/to/opentelemetry-javaagent.jar \
     -Dotel.service.name=consumer-svc \
     -Dotel.traces.exporter=otlp \
     -Dotel.metrics.exporter=otlp \
     -Dotel.logs.exporter=otlp \
     -Dotel.instrumentation.kafka.producer-propagation.enabled=true \
     -Dotel.instrumentation.kafka.experimental-span-attributes=true \
     -Dotel.instrumentation.kafka.metric-reporter.enabled=true \
     -jar /path/to/your/consumer.jar
```

### Step 4: OpenTelemetry Collector Setup

Set up the OpenTelemetry Collector to collect JMX metrics from Kafka and spans from producer and consumer clients, then forward them to SigNoz.

4.1 **Download the JMX Metrics Collector**

Download the JMX metrics collector necessary for Kafka metrics collection. 

You can download the latest `.jar file` for the opentelemetry jmx metrics using 
this [release link](https://github.com/open-telemetry/opentelemetry-java-contrib/releases).

4.2 **Install the OpenTelemetry Collector**

Install the local OpenTelemetry Collector Contrib.


**Using the Binary:**

Download the OpenTelemetry Collector Contrib binary. You can [follow this doc](https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/) to 
download the binary for your Operating System.

Place the binary in the root of your project directory.

**Update config file**

Update you collector config with the follwing:

<Tabs>
<TabItem value="SigNoz Cloud" label="SigNoz Cloud" default>

```yaml
receivers:
  # Read more about kafka metrics receiver - https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkametricsreceiver/README.md
  kafkametrics:
    brokers:
      - localhost:9092
      - localhost:9093
      - localhost:9094
    protocol_version: 2.0.0
    scrapers:
      - brokers
      - topics
      - consumers
  # Read more about jmx receiver - https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/jmxreceiver/README.md
  jmx/1:
    # configure the path where you installed opentelemetry-jmx-metrics jar
    jar_path: path/to/opentelemetry-jmx-metrics.jar #change this to the path to you opentelemetry-jmx-metrics jar file you downloaded above
    endpoint: service:jmx:rmi:///jndi/rmi://localhost:9991/jmxrmi
    target_system: jvm,kafka,kafka-consumer,kafka-producer
    collection_interval: 10s
    log_level: info
    resource_attributes:
      broker.name: broker1
  jmx/2:
    jar_path: ${PWD}/opentelemetry-jmx-metrics.jar
    endpoint: service:jmx:rmi:///jndi/rmi://localhost:9992/jmxrmi
    target_system: jvm,kafka,kafka-consumer,kafka-producer
    collection_interval: 10s
    log_level: info
    resource_attributes:
      broker.name: broker2
exporters:
  otlp:
    endpoint: "ingest.{region}.signoz.cloud:443"
    tls:
      insecure: false
    headers:
      "signoz-access-token": "<SIGNOZ_INGESTION_KEY>"
  debug:
    verbosity: detailed

service:
  pipelines:
    metrics:
      receivers: [kafkametrics, jmx/1, jmx/2]
      exporters: [otlp]
```

</TabItem>

<TabItem value="Self-Hosted" label="Self-Hosted" default>

```yaml
receivers:
  # Read more about kafka metrics receiver - https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/kafkametricsreceiver/README.md
  kafkametrics:
    brokers:
      - localhost:9092
      - localhost:9093
      - localhost:9094
    protocol_version: 2.0.0
    scrapers:
      - brokers
      - topics
      - consumers
  # Read more about jmx receiver - https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/receiver/jmxreceiver/README.md
  jmx/1:
    # configure the path where you installed opentelemetry-jmx-metrics jar
    jar_path: path/to/opentelemetry-jmx-metrics.jar #change this to the path to you opentelemetry-jmx-metrics jar file you downloaded above
    endpoint: service:jmx:rmi:///jndi/rmi://localhost:9991/jmxrmi
    target_system: jvm,kafka,kafka-consumer,kafka-producer
    collection_interval: 10s
    log_level: info
    resource_attributes:
      broker.name: broker1
  jmx/2:
    jar_path: ${PWD}/opentelemetry-jmx-metrics.jar
    endpoint: service:jmx:rmi:///jndi/rmi://localhost:9992/jmxrmi
    target_system: jvm,kafka,kafka-consumer,kafka-producer
    collection_interval: 10s
    log_level: info
    resource_attributes:
      broker.name: broker2
exporters:
   otlp:
     endpoint: "<IP of machine hosting SigNoz>:4317"
     tls:
       insecure: true
  debug:
    verbosity: detailed

service:
  pipelines:
    metrics:
      receivers: [kafkametrics, jmx/1, jmx/2]
      exporters: [otlp]
```

</TabItem>
</Tabs>
The config file sets up the OpenTelemetry Collector to gather Kafka and JVM metrics from multiple brokers(2 brokers in the above config) via Kafka metrics scraping 
and JMX, and exports the telemetry data to SigNoz using the OTLP exporter. 

**Run the collector** 

Run the collector with the above configuration :

```bash
./otelcol-contrib --config path/to/config.yaml
```

</TabItem>


<TabItem value="Strimzi" label="Strimzi">

### Step 1: Strimzi Operator Installation

Install the Strimzi Kafka Operator using Helm:

```bash
helm install strimzi-cluster-operator oci://quay.io/strimzi-helm/strimzi-kafka-operator
```

### Step 2: Deploy Kafka Cluster

Create a Kafka cluster with JMX metrics enabled. Apply the following configuration:

```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: signoz-demo-cluster
spec:
  kafka:
    version: 3.8.0
    replicas: 3
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true
    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
      default.replication.factor: 3
      min.insync.replicas: 2
      inter.broker.protocol.version: "3.8"
    storage:
      type: ephemeral
    metricsConfig:
      type: jmxPrometheusExporter
      valueFrom:
        configMapKeyRef:
          name: kafka-metrics
          key: kafka-metrics-config.yml
  zookeeper:
    replicas: 3
    storage:
      type: ephemeral
  entityOperator:
    topicOperator: {}
    userOperator: {}
```

### Step 3: Configure Metrics Collection

Create and apply the Kafka metrics ConfigMap:

```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: kafka-metrics
  labels:
    app: strimzi
data:
  kafka-metrics-config.yml: |
    # See https://github.com/prometheus/jmx_exporter for more info about JMX Prometheus Exporter metrics
    lowercaseOutputName: true
    rules:
    # Special cases and very specific rules
    - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), topic=(.+), partition=(.*)><>Value
      name: kafka_server_$1_$2
      type: GAUGE
      labels:
        clientId: "$3"
        topic: "$4"
        partition: "$5"
    - pattern: kafka.server<type=(.+), name=(.+), clientId=(.+), brokerHost=(.+), brokerPort=(.+)><>Value
      name: kafka_server_$1_$2
      type: GAUGE
      labels:
        clientId: "$3"
        broker: "$4:$5"
    - pattern: kafka.server<type=(.+), cipher=(.+), protocol=(.+), listener=(.+), networkProcessor=(.+)><>connections
      name: kafka_server_$1_connections_tls_info
      type: GAUGE
      labels:
        cipher: "$2"
        protocol: "$3"
        listener: "$4"
        networkProcessor: "$5"
    - pattern: kafka.server<type=(.+), clientSoftwareName=(.+), clientSoftwareVersion=(.+), listener=(.+), networkProcessor=(.+)><>connections
      name: kafka_server_$1_connections_software
      type: GAUGE
      labels:
        clientSoftwareName: "$2"
        clientSoftwareVersion: "$3"
        listener: "$4"
        networkProcessor: "$5"
    - pattern: "kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+-total):"
      name: kafka_server_$1_$4
      type: COUNTER
      labels:
        listener: "$2"
        networkProcessor: "$3"
    - pattern: "kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+):"
      name: kafka_server_$1_$4
      type: GAUGE
      labels:
        listener: "$2"
        networkProcessor: "$3"
    - pattern: kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+-total)
      name: kafka_server_$1_$4
      type: COUNTER
      labels:
        listener: "$2"
        networkProcessor: "$3"
    - pattern: kafka.server<type=(.+), listener=(.+), networkProcessor=(.+)><>(.+)
      name: kafka_server_$1_$4
      type: GAUGE
      labels:
        listener: "$2"
        networkProcessor: "$3"
    # Some percent metrics use MeanRate attribute
    # Ex) kafka.server<type=(KafkaRequestHandlerPool), name=(RequestHandlerAvgIdlePercent)><>MeanRate
    - pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*><>MeanRate
      name: kafka_$1_$2_$3_percent
      type: GAUGE
    # Generic gauges for percents
    - pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*><>Value
      name: kafka_$1_$2_$3_percent
      type: GAUGE
    - pattern: kafka.(\w+)<type=(.+), name=(.+)Percent\w*, (.+)=(.+)><>Value
      name: kafka_$1_$2_$3_percent
      type: GAUGE
      labels:
        "$4": "$5"
    # Generic per-second counters with 0-2 key/value pairs
    - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+), (.+)=(.+)><>Count
      name: kafka_$1_$2_$3_total
      type: COUNTER
      labels:
        "$4": "$5"
        "$6": "$7"
    - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*, (.+)=(.+)><>Count
      name: kafka_$1_$2_$3_total
      type: COUNTER
      labels:
        "$4": "$5"
    - pattern: kafka.(\w+)<type=(.+), name=(.+)PerSec\w*><>Count
      name: kafka_$1_$2_$3_total
      type: COUNTER
    # Generic gauges with 0-2 key/value pairs
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Value
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        "$4": "$5"
        "$6": "$7"
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Value
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        "$4": "$5"
    - pattern: kafka.(\w+)<type=(.+), name=(.+)><>Value
      name: kafka_$1_$2_$3
      type: GAUGE
    # Emulate Prometheus 'Summary' metrics for the exported 'Histogram's.
    # Note that these are missing the '_sum' metric!
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+), (.+)=(.+)><>Count
      name: kafka_$1_$2_$3_count
      type: COUNTER
      labels:
        "$4": "$5"
        "$6": "$7"
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*), (.+)=(.+)><>(\d+)thPercentile
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        "$4": "$5"
        "$6": "$7"
        quantile: "0.$8"
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.+)><>Count
      name: kafka_$1_$2_$3_count
      type: COUNTER
      labels:
        "$4": "$5"
    - pattern: kafka.(\w+)<type=(.+), name=(.+), (.+)=(.*)><>(\d+)thPercentile
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        "$4": "$5"
        quantile: "0.$6"
    - pattern: kafka.(\w+)<type=(.+), name=(.+)><>Count
      name: kafka_$1_$2_$3_count
      type: COUNTER
    - pattern: kafka.(\w+)<type=(.+), name=(.+)><>(\d+)thPercentile
      name: kafka_$1_$2_$3
      type: GAUGE
      labels:
        quantile: "0.$4"
    # KRaft overall related metrics
    # distinguish between always increasing COUNTER (total and max) and variable GAUGE (all others) metrics
    - pattern: "kafka.server<type=raft-metrics><>(.+-total|.+-max):"
      name: kafka_server_raftmetrics_$1
      type: COUNTER
    - pattern: "kafka.server<type=raft-metrics><>(.+):"
      name: kafka_server_raftmetrics_$1
      type: GAUGE
    # KRaft "low level" channels related metrics
    # distinguish between always increasing COUNTER (total and max) and variable GAUGE (all others) metrics
    - pattern: "kafka.server<type=raft-channel-metrics><>(.+-total|.+-max):"
      name: kafka_server_raftchannelmetrics_$1
      type: COUNTER
    - pattern: "kafka.server<type=raft-channel-metrics><>(.+):"
      name: kafka_server_raftchannelmetrics_$1
      type: GAUGE
    # Broker metrics related to fetching metadata topic records in KRaft mode
    - pattern: "kafka.server<type=broker-metadata-metrics><>(.+):"
      name: kafka_server_brokermetadatametrics_$1
      type: GAUGE
```

Apply the ConfigMap:
```bash
kubectl apply -f kafka-metrics-config.yaml
```

### Step 4: Deploy OpenTelemetry Collector

Install the SigNoz Kubernetes collector with automatic Strimzi discovery:

```yaml
global:
  cloud: <aws, azure, gcp, others>
  clusterName: signoz-demo-cluster
  deploymentEnvironment: production
otelCollectorEndpoint: <Collector Endpoint>
otelInsecure: false
signozApiKey: <API Key Here>
presets:
  otlpExporter:
    enabled: true
  loggingExporter:
    enabled: false
otelDeployment:
  config:
    receivers:
      kafkametrics:
        collection_interval: 10s
        cluster_alias: signoz-demo-cluster1
        brokers:
          - signoz-demo-cluster1-kafka-bootstrap:9092
        protocol_version: 3.0.0
        scrapers:
          - brokers
          - topics
          - consumers
      prometheus:
        config:
          scrape_configs:
            - job_name: 'strimzi-pods'
              scrape_interval: 10s
              kubernetes_sd_configs:
              - role: pod
              relabel_configs:
              - source_labels: [__meta_kubernetes_pod_label_strimzi_io_kind]
                regex: Kafka
                action: keep
```

Deploy the collector:
```bash
helm install my-release signoz/k8s-infra -f ./signoz-k8.yml
```

### Step 5: Deploy Producer and Consumer Applications

Deploy the Kafka producer and consumer applications with OpenTelemetry instrumentation:

**Compile your Java producer and consumer applications:** Ensure your producer and consumer apps are compiled and ready to run.

**Run Producer App with Java Agent:**

```bash
java -javaagent:/path/to/opentelemetry-javaagent.jar \
     -Dotel.service.name=producer-svc \
     -Dotel.traces.exporter=otlp \
     -Dotel.metrics.exporter=otlp \
     -Dotel.logs.exporter=otlp \
     -jar /path/to/your/producer.jar
```

**Run Consumer App with Java Agent:**

```bash
java -javaagent:/path/to/opentelemetry-javaagent.jar \
     -Dotel.service.name=consumer-svc \
     -Dotel.traces.exporter=otlp \
     -Dotel.metrics.exporter=otlp \
     -Dotel.logs.exporter=otlp \
     -Dotel.instrumentation.kafka.producer-propagation.enabled=true \
     -Dotel.instrumentation.kafka.experimental-span-attributes=true \
     -Dotel.instrumentation.kafka.metric-reporter.enabled=true \
     -jar /path/to/your/consumer.jar
```

### Example: Deploying Sample Kafka Producer and Consumer Applications with OpenTelemetry Java Agent

```yaml
# Producer Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-producer1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-producer1
  template:
    metadata:
      labels:
        app: kafka-producer1
    spec:
      containers:
      - name: producer
        image: jaikanthjay46/kafka-consumer-test:latest
        env:
        - name: BOOTSTRAP_SERVERS
          value: "signoz-demo-cluster1-kafka-bootstrap:9092"
        - name: TOPIC
          value: "topic1"
        - name: PARTITION_KEY
          value: "key1"
        - name: DELAY
          value: "10"
        - name: OTEL_SERVICE_NAME
          value: "producer-svc1"
        - name: OTEL_TRACES_EXPORTER
          value: "otlp"
        - name: OTEL_METRICS_EXPORTER
          value: "otlp"
        - name: OTEL_LOGS_EXPORTER
          value: "otlp"
        - name: OTEL_EXPORTER_OTLP_LOGS_ENDPOINT
          value: "http://my-release-k8s-infra-otel-agent:4318/v1/logs"
        - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
          value: "http://my-release-k8s-infra-otel-agent:4318/v1/metrics"
        - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
          value: "http://my-release-k8s-infra-otel-agent:4318/v1/traces"
---
# Consumer Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka-consumer1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka-consumer1
  template:
    metadata:
      labels:
        app: kafka-consumer1
    spec:
      containers:
      - name: consumer
        image: jaikanthjay46/kafka-consumer-test:latest
        env:
        - name: BOOTSTRAP_SERVERS
          value: "signoz-demo-cluster1-kafka-bootstrap:9092"
        - name: CONSUMER_GROUP
          value: "cg1"
        - name: TOPIC
          value: "topic1"
        - name: OTEL_SERVICE_NAME
          value: "consumer-svc"
        - name: OTEL_TRACES_EXPORTER
          value: "none"
        - name: OTEL_METRICS_EXPORTER
          value: "none"
        - name: OTEL_LOGS_EXPORTER
          value: "none"
        - name: OTEL_EXPORTER_OTLP_LOGS_ENDPOINT
          value: "http://my-release-k8s-infra-otel-agent:4318/v1/logs"
        - name: OTEL_EXPORTER_OTLP_METRICS_ENDPOINT
          value: "http://my-release-k8s-infra-otel-agent:4318/v1/metrics"
        - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
          value: "http://my-release-k8s-infra-otel-agent:4318/v1/traces"
```

Apply the deployments:
```bash
kubectl apply -f kafka-applications.yaml
```
Please take note of otel service name `my-release-k8s-infra-otel-agent` in the above yaml file, you will need to replace it with your otel service name.
It's of the format `<helm-release-name>-k8s-infra-otel-agent`.

Continue to visualize your data in SigNoz as described in the previous sections.

</TabItem>

<TabItem value="Amazon MSK" label="Amazon MSK" default>


This guide assumes you have an MSK cluster you want to monitor. If you don't have one, you can create one using the [Amazon MSK console](https://docs.aws.amazon.com/msk/latest/developerguide/create-cluster.html).


### Step 1: Enable  Monitoring

- Go to the Amazon MSK console
- Select the cluster
- Click on the `Actions` > `Edit Monitoring`
- Configure the following settings as in the screenshot below:

<figure data-zoomable align="center">
    <img
      src="/img/docs/messaging-queues/kafka/msk-monitoring-settings.webp"
      alt="MSK Monitoring Settings"
    />
    <figcaption>
      <i>
        MSK Monitoring Settings
      </i>
    </figcaption>
</figure>


### Step 2: Configure OpenTelemetry Collector

Create a configuration file for the OpenTelemetry Collector:

```yaml
receivers:
  prometheus:
    config:
      scrape_configs:
        - job_name: 'broker'
          file_sd_configs:
          - files:
            - 'targets.json'

exporters:
  otlp:
    endpoint: "<YOUR_SIGNOZ_CLOUD_URL>"
    tls:
      insecure: false
    headers:
      "signoz-access-token": "<SIGNOZ_INGESTION_KEY>"

service:
  pipelines:
    metrics:
      receivers: [prometheus]
      processors: [batch]
      exporters: [otlp]
```


#### Targets JSON File 
The `targets.json` file contains the list of brokers and their ports. You can get all the broker dns by:
- Navigating to the Amazon MSK console
- Selecting the cluster
- Clicking on the `Properties` tab

You will find the `Broker DNS` under the `Broker details` section in the `Endpoints` column.

```json
[
  {
    "labels": {
      "job": "jmx"
    },
    "targets": [
      "broker_dns_1:11001",
      "broker_dns_2:11001",
      .
      .
      .
      "broker_dns_N:11001"
    ]
  },
  {
    "labels": {
      "job": "node"
    },
    "targets": [
      "broker_dns_1:11002",
      "broker_dns_2:11002",
      .
      .
      .
      "broker_dns_N:11002"
    ]
  }
]
```


### Step 3: Run OpenTelemetry Collector

Create an EC2 instance in the same VPC as your MSK cluster.

1. Install the OpenTelemetry Collector:
You can [follow this doc](https://signoz.io/docs/tutorial/opentelemetry-binary-usage-in-virtual-machine/) to download the binary for your Operating System.

Place the binary in the root of your project directory.

2. Start the collector:
```bash
./otelcol --config config.yaml
```

### Step 4: Deploy Applications

**Compile your Java producer and consumer applications:** Ensure your producer and consumer apps are compiled and ready to run.

**Run Producer App with Java Agent:**

```bash
java -javaagent:/path/to/opentelemetry-javaagent.jar \
     -Dotel.service.name=producer-svc \
     -Dotel.traces.exporter=otlp \
     -Dotel.metrics.exporter=otlp \
     -Dotel.logs.exporter=otlp \
     -jar /path/to/your/producer.jar
```

**Run Consumer App with Java Agent:**

```bash
java -javaagent:/path/to/opentelemetry-javaagent.jar \
     -Dotel.service.name=consumer-svc \
     -Dotel.traces.exporter=otlp \
     -Dotel.metrics.exporter=otlp \
     -Dotel.logs.exporter=otlp \
     -Dotel.instrumentation.kafka.producer-propagation.enabled=true \
     -Dotel.instrumentation.kafka.experimental-span-attributes=true \
     -Dotel.instrumentation.kafka.metric-reporter.enabled=true \
     -jar /path/to/your/consumer.jar
```

Continue to the visualization step to view your metrics in SigNoz.

</TabItem>

</Tabs>

### Visualize data in SigNoz

**To see your Producer and Consumer app traces:**

- Head over to **Services** tab in your SigNoz instance.
- You should be able to see your apps in the list along with some [application metrics](https://signoz.io/docs/userguide/metrics/).
- Click on the service of your choice to see more detailed application metrics and [related traces](https://signoz.io/docs/userguide/traces/).

<figure data-zoomable align="center">
    <img
      src="/img/docs/messaging-queues/kafka/kafka-services-tabs.webp"
      alt="Kafka Producer app in Services Tab of SigNoz"
    />
    <figcaption>
      <i>
        Kafka Producer app in Services tab of SigNoz
      </i>
    </figcaption>
</figure>

**To see the Kafka Metrics:**

- Head over to **Messaging Queues** tab in your SigNoz instance.
- You will get different Options like **Consumer Lag View** etc., to see various kafka related metrics.

<figure data-zoomable align="center">
    <img
      src="/img/docs/messaging-queues/kafka/kafka-metrics.webp"
      alt="Messaging Queues tab in SigNoz"
    />
    <figcaption>
      <i>
        Messaging Queues tab in SigNoz
      </i>
    </figcaption>
</figure>
