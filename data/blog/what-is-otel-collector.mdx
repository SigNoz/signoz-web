---
title: What is the OpenTelemetry Collector?
slug: what-is-otel-collector
date: 2025-09-03
tags: [Observability, OpenTelemetry]
authors: [elizabeth_mathew]
description: The OpenTelemetry Collector is a vendor-neutral service that can receive, process, and export telemetry data. It is a crucial component of the OpenTelemetry ecosystem.
image: /img/blog/2025/07/dunning-kruger.webp
keywords: [observability, opentelemetry, devops, sre, developer, product engineer, otel collector, receiver, processor, exporter]
---

If you are new to OpenTelemetry, I highly recommend reading [this](https://signoz.io/blog/what-is-opentelemetry/). If you have already wrapped your head around the basics, let’s explore OpenTelemetry Collector.

## What is the OpenTelemetry Collector?

In very simple words, the collector is an agent that sits between your instrumented services, collects the telemetry data that is emitted by the services, processes it and sends it to an observability backend.  
The most textbook definition of this [and what’s present in the official docs] is that the OpenTelemetry Collector offers a *vendor-agnostic implementation of how to receive, process and export telemetry data*. Vendor-agnostic means you can plug into any observability backend of your choice and switch it up as you prefer, without changing your application code. No vendor lock-in. 

<OtelCollectorFlow />

Apart from acting as a buffer, the collector also acts as a *translation layer;* it supports the native OpenTelemetry Protocol [OTLP] for ingesting data, but can also accept other formats [like Jaeger trace data or Prometheus metrics] and translate them on the fly.

## Architecture and Parts of OpenTelemetry Collector

OpenTelemetry collector runs various *pipelines,* which are responsible for *transporting* your telemetry data from the instrumented service to the backend. Each pipeline is built with *receivers, processors* and *exporters* on which we will discuss elaborately.

Let’s break them down.

### Receivers

These are how data *gets into* ****the Collector.  Receivers are configured to accept telemetry data in various formats. They can be:

- **Push-based:** Listening on a port for data to be sent to them [e.g., the OTLP receiver].
- **Pull-based:** Actively scraping an endpoint for data [e.g., the Prometheus receiver].

The Collector supports many protocols out of the box.  

The Collector supports many receiver protocols/formats. For example, the native OTLP is commonly used, but you can also receive Jaeger, Prometheus, Fluentbit, and other formats. Each receiver knows how to *translate* the incoming data into the Collector’s internal format, ready for processing.

### Processors

Processors sit in the middle of the pipeline and *modify* the telemetry as it flows through the pipeline. They possess powerful capabilities like, 

- **Batch data** for more efficient transport.
- **Filter out noise** by dropping telemetry from specific sources.
- **Sample your traces** to manage volume and control costs.
- **Add new attributes** to all your data [e.g., cloud.region: us-east-1].
- **Protect sensitive information** by masking personally identifiable information [PII].
- **Control memory usage** to prevent the Collector from crashing under heavy load.

etc..

Although processors are *optional*, they are one of the most essential components for collectors.

### Exporters

Exporters are responsible for *sending out* the processed (if) telemetry to your chosen backend(s).  It’s also versatile in a way that you can configure *multiple exporters in a single pipeline*, enabling you to send the *same data to multiple backends in parallel* if desired. This allows you to, for example, send the same trace data to both Jaeger for internal debugging and to a commercial vendor for long-term storage and analysis, all from one source.

## When to use the OpenTelemetry Collector?

It’s *not necessary* to have a collector for every use case involving OpenTelemetry because, like any program, the *Collector consumes resources and requires management*. If it’s not providing any value, you don’t have to run it. You can always add Collectors later if the need arises.

If the telemetry being emitted requires little to no processing, it may make sense to connect the SDKs directly to the backend, without a Collector.

Consider adding a Collector to your architecture if you find yourself in any of these situations**:**

- **When you need to scale.** If you have more than a few microservices, a Collector provides a single point of management for configuration. It's *much easier to update one Collector* than to redeploy dozens of applications.
- **When you need to do heavy-duty processing.** If you want to *filter* out noisy data, *sample* traces to control costs, or *enrich* all your telemetry with common metadata [like a cluster name or cloud region], the Collector is the place to do it without slowing down your applications.
- **When you want to offload responsibility.** The Collector can handle tasks like data batching, retry logic, and authentication with your backend vendor, which simplifies your application code and reduces its resource footprint.
- **When you need to collect infrastructure or third-party data.** The Collector is essential for gathering telemetry from sources that *you can't instrument with an SDK*. This is done using specific **receivers**. Common examples include:
    - Collecting *host metrics* [CPU, memory, disk, network].
    - Scraping metrics from a Prometheus endpoint.
    - Receiving traces from applications instrumented with Jaeger or Zipkin.
    - Tailing log files from disk.
    - Querying a database for specific metrics.
- **When you have a "many-to-many" data problem.** If you have **a lot of receivers** [ingesting data from OTLP, Jaeger, Prometheus, etc.] and need to send data to *a lot of exporters* [a vendor platform, an internal database, a logging system], the Collector's pipeline model is designed to manage this complexity centrally.

Conversely, if you have a single application sending data to a single backend with no need for advanced processing, the simpler direct-from-SDK approach is perfectly fine.

## Setup and Configuration

Although the concept and the various parts can sound *intimidating*, the implementation is *relatively simple*. The entire process boils down to three key steps:

1. **Install the Collector** – Available as binaries, Docker images, or Kubernetes deployments.
2. **Define a Config** – Use a YAML file to wire receivers → processors → exporters into pipelines. 
3. **Point Your App at the Collector** – Configure your application’s OpenTelemetry SDK to send data to the Collector instead of directly to a backend.

To make this concrete, here is what a typical configuration file looks like.

```yaml
# 1. RECEIVERS: How data gets IN
receivers:
  otlp:
    protocols:
      grpc:
      http:
  prometheus:
    config:
      scrape_configs:
        - job_name: "demo-app"
          static_configs:
            - targets: ["localhost:8080"]

# 2. PROCESSORS: What happens to data in the middle
processors:
  batch: {}

# 3. EXPORTERS: How data gets OUT
exporters:
  otlp:
    endpoint: OTLP_ENDPOINT
    tls:
      insecure: true
  logging:
    loglevel: debug

# 4. SERVICE: Where you define the pipelines
service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp, logging]
    metrics:
      receivers: [otlp, prometheus]
      processors: [batch]
      exporters: [otlp, logging]
    logs:
      receivers: [otlp]
      processors: [batch]
      exporters: [otlp, logging]

```

For a detailed step-by-step guide [with configs and code], check out our [OpenTelemetry Collector setup tutorial.](https://signoz.io/blog/opentelemetry-collector-complete-guide/#step-by-step-configuration-tutorial)
