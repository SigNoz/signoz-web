---

title: Deploying and Scaling OpenTelemetry in Production NextJS Apps
slug: opentelemetry-nextjs-production
date: 2025-06-23
tags: [OpenTelemetry, NextJS, Observability, SigNoz, JavaScript]
authors: [yuvraj]
description: Learn how to deploy OpenTelemetry-instrumented Next.js apps to production with Vercel or self-hosted infra. This guide covers collector vs direct exporter setups, alerting, sampling, and data sanitization best practices.
image: /img/blog/2022/04/opentelemetry_nextjs_cover.webp
keywords: [nextjs, opentelemetry, production monitoring, collector vs exporter, sigznoz, observability, javascript, tracing, sampling, alerting]

---

## Deploying the Instrumented NextJS App to Production

Once you‚Äôve instrumented your Next.js app with OpenTelemetry, the next step is getting it into production. Whether you‚Äôre shipping via Vercel or running your own infra, the setup has some key differences worth noting.

### Option 1: Deploying on Vercel

If you‚Äôre using Vercel, good news ‚Äî OpenTelemetry just works.

As per the <a href="https://nextjs.org/docs/app/guides/open-telemetry#deploying-on-vercel" rel="noopener noreferrer nofollow" target="_blank">Next.js docs</a>, no extra config is needed. Vercel supports OpenTelemetry natively for both Node and Edge runtimes.

**Why Vercel works well:**

- No setup headaches ‚Äî just deploy and trace
- Handles scaling and cold starts for you
- Great for hybrid apps (Edge + Node)
- Built-in support for observability providers like SigNoz, Datadog, and more

**Deploy flow:**

```bash
vercel deploy
```

If your `instrumentation.ts` is set up locally, it‚Äôll work in production too.

**Env vars required:**

```bash
OTEL_EXPORTER_OTLP_ENDPOINT=https://ingest.YOUR-REGION.signoz.cloud:443
OTEL_EXPORTER_OTLP_HEADERS=signoz-access-token=your-token
```

---

### üõ†Ô∏è Option 2: Self-Hosting (Docker / K8s)

Prefer full control? Running your app on your own infra with Docker or Kubernetes gives you more power ‚Äî but also more responsibility.

**Why self-host:**

- Total control over observability data and infra
- More flexibility in how you configure the collector
- Good for large-scale or compliance-sensitive workloads

**What to watch out for:**

- You‚Äôll manage scaling and HA yourself
- Secure your telemetry data pipeline
- Set up alerting/monitoring for the collector itself
- Allocate ops time ‚Äî especially if you‚Äôre pushing serious traffic

### TL;DR

|  | **Vercel** | **Self-Hosted** |
| --- | --- | --- |
| Setup | ‚ö° Super simple | üõ†Ô∏è Manual infra setup |
| Scale | üìà Auto (serverless) | üìä You manage it |
| Observability | üîå Built-in integrations | üîß Fully customizable |
| Best for | Fast-moving teams, hybrid apps | Teams needing full control or compliance |

---

Pick what fits your team‚Äôs speed, scale, and infra appetite. Either way, with OpenTelemetry and SigNoz in place, you‚Äôre set up to ship and debug confidently in production.

## 2. Collector vs Direct Exporter

Choosing between an OpenTelemetry **Collector** and a **Direct Exporter** setup is one of the most important architectural decisions you‚Äôll make. It affects how flexible, scalable, and robust your observability pipeline is ‚Äî especially in production.

---

### Collector Approach (Recommended)

This is the most flexible and production-friendly setup. Instead of sending telemetry data straight to your observability backend, your app sends it to an OpenTelemetry **Collector**. The collector then processes, enriches, and routes it as needed.

### Architecture:

<Figure src="/img/blog/2025/06/opentelemetry-nextjs-production-CleanShot_2025-06-20_at_15.17.002x.webp" alt="" caption="" />### Why use a Collector?

- **Vendor-agnostic** ‚Äî Switch observability backends without touching app code
- **Data processing** ‚Äî Add batching, filtering, sampling, enrichment
- **Fan-out** ‚Äî Send data to multiple backends at once (e.g., SigNoz + Prometheus)
- **Better performance** ‚Äî Reduces load on your app by buffering and batching
- **Centralized auth** ‚Äî Manage credentials and scrub sensitive data in one place
- **More reliable** ‚Äî Retry and buffer logic during backend downtime

### Example Collector Config

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 1s
    send_batch_size: 1024
  attributes:
    actions:
      - key: environment
        value: production
        action: upsert
  filter:
    spans:
      exclude:
        match_type: regexp
        attributes:
          - key: http.target
            value: "/health.*"

exporters:
  otlp/signoz:
    endpoint: "ingest.YOUR-REGION.signoz.cloud:443"
    headers:
      signoz-access-token: ${SIGNOZ_TOKEN}
  prometheus:
    endpoint: "0.0.0.0:8889"

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [batch, attributes, filter]
      exporters: [otlp/signoz]
    metrics:
      receivers: [otlp]
      processors: [batch, attributes]
      exporters: [otlp/signoz, prometheus]

```

### Your Next.js App Config (Collector Setup):

```tsx
// instrumentation.ts
import { registerOTel } from '@vercel/otel'

export function register() {
  registerOTel({
    serviceName: 'nextjs-production-app',
    endpoint: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318',
  })
}

```

---

### Direct Exporter Approach

With this approach, your app sends telemetry data straight to your observability backend ‚Äî no collector in between.

### Architecture

<Figure src="/img/blog/2025/06/opentelemetry-nextjs-production-CleanShot_2025-06-20_at_15.18.262x.webp" alt="" caption="" />### Pros:

- Simpler to set up ‚Äî fewer moving parts
- Lower latency ‚Äî no intermediate hop
- Great for demos or smaller apps

### Cons:

- No data enrichment, filtering, or custom processing
- Hard to route data to multiple platforms
- More resource usage inside your app
- No retry/buffering logic ‚Äî dropped data on failure

### üîß Direct Exporter Example:

```tsx
// instrumentation.ts
import { registerOTel } from '@vercel/otel'
import { OTLPHttpJsonTraceExporter } from '@vercel/otel'

export function register() {
  registerOTel({
    serviceName: 'nextjs-production-app',
    traceExporter: new OTLPHttpJsonTraceExporter({
      url: 'https://ingest.us.signoz.cloud:443/v1/traces',
      headers: {
        'signoz-access-token': process.env.SIGNOZ_INGESTION_KEY || '',
      },
    }),
  })
}

```

---

### TL;DR - Which One to Choose?

|  | **Collector**  (Recommended) | **Direct Exporter** |
| --- | --- | --- |
| Vendor flexibility |  High | ‚ùå Low |
| Reliability |  Retry + buffer | ‚ùå Risk of drop |
| Processing |  Batch, filter, enrich | ‚ùå None |
| Infra required | ‚ö†Ô∏è Needs extra setup |  Minimal |
| Multi-backend output |  Easy fan-out | ‚ùå Not supported |
| Best for | Prod, large apps | Dev, small projects |

---

In short: **use a collector** in production if you care about reliability, flexibility, or long-term maintainability. Use direct exporters only for quick setups or demos where speed > control.

## Setting Up Alerts

Monitoring without alerts is like a smoke detector without a battery ‚Äî it might know something‚Äôs wrong, but no one hears it.

With SigNoz, you can set up production-grade alerts on latency, errors, external APIs, database health, and more ‚Äî all powered by auto-generated metrics from your OpenTelemetry traces.

### Understanding SigNoz APM Metrics

SigNoz automatically generates structured metrics from your traces. No extra config needed.

**Common Metrics You'll Use:**

| Metric | Description |
| --- | --- |
| `signoz_calls_total` | Total number of service calls |
| `signoz_latency_bucket` | Latency histogram buckets (used for P99, P95, etc) |
| `signoz_latency_sum/count` | Raw latency values |
| `signoz_db_latency_sum/count` | Database call timings |
| `signoz_external_call_latency_sum/count` | External API timings |

You can filter any of these by:

- `service_name`
- `operation` (e.g. `GET /api/login`)
- `http_status_code`
- `deployment_environment`

### Alert Example: High P99 Latency

Track if any of your key endpoints cross the 2s P99 threshold ‚Äî a common user experience red flag.

**SigNoz UI Configuration:**

<Figure src="/img/blog/2025/06/opentelemetry-nextjs-production-image.webp" alt="" caption="" />- **Metric**: `signoz_latency_bucket`
- **Aggregation**: P99
- **Filter**: `service.name = nextjs-observability-demo`
- **Group By**: `operation`
- **Threshold**: Above 2000ms for 5 minutes
- **Severity**: Critical

> This catches slow endpoints after deployments, during traffic spikes, or when a downstream service misbehaves.
> 

## Testing the Alert

Generate alerts manually to verify setup:

```bash
# Simulate high latency
curl "http://localhost:3000/api/external?service=slow"

# Simulate 404 errors
for i in {1..20}; do curl -s -o /dev/null -w "%{http_code}\n" http://localhost:3000/not-found; done

# Simulate DB alert (example)
curl http://localhost:3000/api/heavy-database-query

```

<Figure src="/img/blog/2025/06/opentelemetry-nextjs-production-image%201.webp" alt="" caption="" />Check:

- SigNoz ‚Üí Alerts ‚Üí **Firing Alerts**
- Notification channels (Slack, email, PagerDuty)
- Auto-resolve when things calm down

<Figure src="/img/blog/2025/06/opentelemetry-nextjs-production-image%202.webp" alt="" caption="" />## Maintaining Alert Hygiene

Set it and forget it? Nope. Keep alert fatigue away with regular tuning:

- **Weekly**: Review false positives
- **Monthly**: Adjust thresholds for seasonal traffic
- **Quarterly**: Review what you‚Äôre not monitoring
- **After Incidents**: Patch gaps based on what was missed

Tune it:

- Raise thresholds if too noisy
- Group by operation to reduce clutter
- Shorten time windows for fast-reacting alerts

A good alerting setup keeps you **ahead** of issues ‚Äî not reacting to user complaints. By wiring up latency, errors, external dependencies, and DB performance, you‚Äôve got a solid first line of defense.

Next time something breaks at 2AM, you‚Äôll already know why.

## Sampling Strategy for Production

Once you instrument your app with OpenTelemetry, you‚Äôll quickly realize it can generate **a lot** of data. That‚Äôs where sampling comes in ‚Äî it lets you retain visibility without flooding your backend (or your bill).

---

### What Sampling Actually Means

Sampling means: ‚ÄúDon‚Äôt send every trace, just enough to get the full picture.‚Äù

- **Sampled traces** ‚Üí Get processed and sent to your observability backend
- **Unsampled traces** ‚Üí Dropped early to save on processing and storage

If you‚Äôre getting thousands of requests per second, you probably don‚Äôt need all of them to know how your app is doing.

### Why You Should Care

- **Lower costs** ‚Äî Send less data to SigNoz or any observability backend
- **Less noise** ‚Äî Focus on what's actually interesting (e.g., errors)
- **Better performance** ‚Äî Reduce tracing overhead on your app
- **Representative insight** ‚Äî You still see meaningful patterns

### When to Use Sampling

Use it if:

- You‚Äôre seeing 1,000+ traces/sec
- Most traffic is routine and low-value
- You want to prioritize important traces (errors, latency, critical flows)

Avoid it if:

- You have very low traffic
- Compliance requires full visibility
- You don‚Äôt have meaningful sampling rules yet

## Head vs Tail Sampling

There are two main ways to decide what gets sampled:

### Head Sampling

This makes the decision **when the trace starts**. Lightweight and works with `@vercel/otel`.

```tsx
import { registerOTel } from '@vercel/otel';
import { TraceIdRatioBasedSampler } from '@opentelemetry/sdk-trace-base';

registerOTel({
  serviceName: 'nextjs-app',
  traceSampler: new TraceIdRatioBasedSampler(0.1), // sample 10%
});
```

Or use environment variables (recommended for production):

```bash
OTEL_TRACES_SAMPLER=probabilistic
OTEL_TRACES_SAMPLER_ARG=0.1
```

- Fast
- Low overhead
- Can‚Äôt see the whole trace (might miss errors late in the flow)

### Tail Sampling

This happens after the full trace is collected ‚Äî so it‚Äôs smarter, but heavier.

**Set up in the OpenTelemetry Collector:**

```yaml
processors:
  tail_sampling:
    decision_wait: 10s
    policies:
      - name: errors
        type: status_code
        status_code: { status_codes: [ERROR] }

      - name: high_latency
        type: latency
        latency: { threshold_ms: 1000 }
        sampling_percentage: 50

      - name: normal_traffic
        type: probabilistic
        probabilistic: { sampling_percentage: 5 }

```

- Smarter decisions
- Always catch errors or slow traces
- Needs the collector and more infra

Start simple: head sampling via environment variables.

Grow later: add tail sampling when you need more precision.

Review regularly: don‚Äôt let old sampling rules hide new problems.

Observability ‚â† ‚Äúlog everything‚Äù ‚Äî it‚Äôs about getting the **right** signals. Sampling helps you do that without going broke.

## Hiding Sensitive Data

Telemetry can accidentally capture sensitive stuff‚Äîemails, tokens, IPs, etc. You‚Äôre responsible for making sure that doesn‚Äôt happen.

### What to Watch For

Common leaks in Next.js apps:

```
user.email = "john@example.com"
headers.authorization = "Bearer xyz"
http.url = "/reset?token=abc"
process.env.DATABASE_URL
```

### Step 1: Don‚Äôt Collect It

```
registerOTel({
  attributes: {
    'deployment.environment': process.env.NODE_ENV,
    'service.version': process.env.npm_package_version,
    // ‚ùå Don‚Äôt include user emails or IDs
  },
  instrumentationConfig: {
    fetch: {
      ignoreUrls: [/token=/, /session=/],
      dontPropagateContextUrls: [/analytics\./],
      resourceNameTemplate: "{http.method} {http.host}"
    }
  }
})

```

### Step 2: Scrub at the Collector

```yaml
processors:
  attributes/sanitize:
    actions:
      - key: user.email
        action: delete
      - key: http.request.header.authorization
        action: delete
      - key: http.url.query
        action: delete
  redaction/allowlist:
    allowed_attributes:
      - http.method
      - http.status_code
      - http.route
      - service.name
      - duration

```

### Step 3: Validate No Leaks

```sql
# Should return 0
count(rate(traces_total{user_email!=""}[5m]))

# No auth tokens
count(rate(traces_total{http_request_header_authorization!=""}[5m]))
```